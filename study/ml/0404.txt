기울기

y = wx + b -> 기울기 w : x가 1만큼 증가했을때 결과값이 a많큼 증가한다

feature가 늘어나면 w는 각 featurbe에 해당하는 기울기를 모두 가짐


랜덤 함수 -> seed : 고정시키면 동일한 순서로 나옴

LinearRegression : 선형회귀


Ridge : 모델을 단순하게 만들고 훈련 세트에 대한 성능 차이를 절충 (트레이닝 낮추고 테스트 높임)

=> 과대적합을 완화시켜줌, 일반화에 효과적인 모델

alpha : 모델을 얼마나 단순화 시킬지의 매개변수 alpha가 높으면 훈련 세트가 낮아지지만 일반화에는
         더 효율적이다. 기본값은 1.0 값을 너무 줄이면 과대적합, 너무 높이면 과소적합


라소 : 리지 처럼 계수를 0에 가깝게 만들려고 함,
        L1규제를 사용함으로 실제 특정 계수는 0이되고 즉 모델에서 제외되는 feature가 생긴다
        alpha값 줄이고 max_iter(반복 실행되는 최대 횟수) 값을 늘려서 조절

        alpha값을 줄이면 모델의 복잡도는 증가하여 훈련 세트와 테스트 세트의 성능이 좋아짐
        성능은 리지와 비슷하지만 사용된 특성이 104개중 33개라 모델을 분석하기가 더 쉬움
        alpha값을 너무 줄이면 과대적합이라 LinearRegression과 비슷해짐



--> 일반적으로는 리지 > 라소 
     feature가 많고 그 중 일부가 중요하다면 라소 > 리지


---------------------------------------------------------------------------------------------------

Decision Tree -> 스무고개


conda install graphviz

pip install graphviz

conda env list -> 자기 가상환경 위치 조회


C:\Users\BIT\Anaconda3\envs\ml-env -> 내 가상환경 위치

C:\Users\BIT\Anaconda3\envs\ml-env\Library\bin\graphviz 주소 복사


(설정 - 시스템 - 정보 - 고급시스템 -) 환경변수 - 시스템변수 - 새로만들기



트리 깊이 제한 -> 과대적합이 줄어듬



https://github.com/rickiepark/intro_ml_with_python_2nd_revised/blob/main/data/ram_price.csv

-> Raw -> 다른이름 저장 -> ml>data생성 & 저장

