global minimum : 목적지

local minima : 여러개 있음

learning rate : 학습률 (보폭)

에타 : n비슷한 모양 


w.shape  : m x n

b.shape : n개



Numpy - 브로드캐스팅 연산 -> 두 개의 행렬 연산을 할때 알아서 배열을 맞춰서 연산함


--------------------------------------------------------------------------------------------

input_data = np.arange(0, np.pi*2, 0.1)
correct_data = np.sin(input_data)
input_data = (input_data - np.pi) / np.pi

-> 데이터 전처리 ( - 1.0 ~ 1.0 으로 변환)


n_in = 1
n_mid = 3
n_out = 1

-> 입력 , 은밀층, 출력의 뉴런 수 정의


wb_width = 0.01
eta = 0.1 
epoch = 2001
interval = 200

-> epoch = 반복량 , interval = 중간 확인 , eta : 학습률



n_upper : 이전 노드 수 

n  : 현재 자기 노드 수



init : 가중치행렬 만들기

foward : 순전파

grad_w : 가중치 오차

update : 출력층


MiddleLayer : 은닉층



입력 들어오면 정답과 비교하여 오차를 줄여가는 것 : 딥 러닝

-> 일련의 과정 = 학습된 모델



-- 파라미터

가중치 

바이오스

// 하이퍼 파라미터 (모델의 성능을 높이기 위해 임의로 조정해야함)

에타 : 학습률

모멘텀 