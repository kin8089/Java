wine -> fit을 history 객체에 저장 , 에포크 3500, 배치 사이즈 500으로 상향

y_vloss = history.history['val_loss']
y_acc = history.history['accuracy']

x_len = np.arange(len(y_acc))
plt.plot(x_len, y_vloss, 'o', c='red', markersize=3)
plt.plot(x_len, y_acc, 'o', c='blue', markersize=3)
plt.show()



-- 성능이 개선되지 않으면 훈련 종료 -> early_stop

wine 복사 

from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping


early_stop = EarlyStopping(monitor='val_loss', patience=100) -> 컴파일 파트에 추가

=> 100번 이상 연속으로 성능이 좋아지지 않으면 훈련 종료

fit 파트의 callbacks에 early_stop 추가 후 실행

=> 에포크 2000번 줬지만 801번 반복 후 종료 됨



-- 15장 housing

노트북 생성 -> 초기 설정 

csv = comma seperated value (컴마로 구분 됨) : type = text(ASCKI, unicode)

housing 파일은 컴마가 아니라 띄어쓰기로 구분해놓음(whitespace)

= > df = pd.read_csv('../data/housing.csv', delim_whitespace=True, header=None)



출력 -> 입력값의 최소 2배 이상 잡아줘야 한다, 보통 짝수 and 8의 배수를 넣는다

ex) input_dim = 13  ->  (최소 26이상)출력= 30  -> 은닉층(두번째) 출력= 6

model = models.Sequential()
model.add(layers.Dense(30, input_dim=13, activation='relu'))
model.add(layers.Dense(6, activation='relu'))
model.add(layers.Dense(1))			-> sigmoid사용 안함
model.summary()


loss = 전체 오차값 (정답과 결과값의 차이)

y_prediction = y_prediction.flatten() -> .flatten() = 기존의 2차원 데이터를 1차원 데이터로 변경 (넘파이 배열 함수)

에포크 마다의 훈련 손실값 (loss)
에포크 마다의 훈련 정확도 (acc)
에포크 마다의 검증 손실값 (val_loss)
에포크 마다의 검증 정확도 (val_acc)


seed값을 주면 출력이 제대로 나오지 않아서 seed값들을 전부 주석 or 삭제


-------------------------------------------------------------------------------------------------------------------

mnist 

-- mnist import 시키기

from tensorflow.keras.datasets import mnist
from tensorflow.keras import utils


-- mnist 데이터 불러오기 

(X_train, y_train), (X_test, y_test) = mnist.load_data()

print(X_train.shape) = (60000, 28, 28) -> 60000개의 데이터가 28 x 28로 이루어짐



plt.imshow(X_train[0], cmap='Greys') -> 데이터를 그래프로 출력

for x in X_train[0]:
    for i in x:
        sys.stdout.write('%d\t' % i)
    sys.stdout.write('\n')

-> 그래프로 그린 데이터를 나열


X_train = X_train.reshape(X_train.shape[0], 28*28)	// 3차원 데이터를 2차원으로 만듬
X_train = X_train.astype(float)
X_train = X_train / 255	// 0~1 사이의 값으로 스케일링

X_test = X_test.reshape(X_test.shape[0], 28*28).astype(float) / 255

-> 입력값에 넣기 위해 조정

--------------------------------------------------------------------------------------------------------------------

mnist 복사 

다중분류 = softmax 사용 , loss='categorical_crossentropy'

model = models.Sequential()
model.add(layers.Dense(512, input_dim=28*28, activation='relu'))
model.add(layers.Dense(10, activation='softmax'))
model.summary()


validation_split -> 훈련데이터의 일정량을 validation(검증)
validation_data -> 지정량 만큼 validation


-------------------------------------------------------------------------------------------------------------------

-- 영역 처리 

Convolution : 공간 영역 기반 처리 

-> Blurring : 영상을 밝게 수정하며 흐리게 처리하는 기법

	화소값이 급격하게 변하는 부분들을 감소시켜 점진적으로 변하게 만듬

	ex) 90 200 -> 99 104


-> Sharpening : 블러 반대


--> Edge : 화소값이 급격하게 변하는 부분


--> Filtering : Mean


-- CNN


X_train = X_train.reshape(X_train.shape[0], 28, 28, 1).astype(float) / 255

-> 이미지 그대로 넣어줄거라 채널 영역이 추가로필요 -> 1 추가 (4차원)



model.add(layers.Conv2D(32, kernal_size=(3,3), input_shape=(28,28,1, activation='relu'))

-> kernal = mask ( 가중치 : 학습 할 파라미터)

    Conv2D = dim : shape

    Conv2D = 1장의 이미지를 출력값 만큼 나눠준다 ex) 32 -> 1장의 이미지 받아서 32장으로 출력

    3 x 3 kernal 을 32장 준비해놓고 들어오는 이미지에 각각 가중치로 계산 -> 각각의 픽셀값을 relu 에 통과

    28 x 28 -> 26 x 26으로 변함 = kernal은 중심값을 변동시킨다 -> 상하좌우가 빠짐



model.add(layers.Maxpooling2D(pool_size=2)) 

-> Maxpooling2D : size x size 마스크를 만들어 최댓값을 뽑아냄
	
	-> ex) 4 x 4 이미지에 적용하면 최대값을 뽑아내서 2 x 2 이미지로 바뀜 (사이즈가 1/4로 줄어듬)
	-> 사진의 물리적 사이즈를 줄이면서 특징만 남긴다



model.add(layers.Dropout(0.25))

-> Dropout : 배치마다 지정 값만큼 필터링(못넘어오게 막음) -> 과대적합이 많이 줄어든다



32 * 9 + 32	320

64 * 9 + 64	640

128 * 9 + 128	1280

128 * 9 + 128	1280

512 * 128 + 512	


390 372 104 9 = 875