spark.apache.org
hadoop.apache.org


-- single 모드 

ubuntu - Hyperion



hadoop : apache의 메인 프로젝트 중 하나, 자바로 구현됨

대량의 데이터를 처리하는 분산 컴퓨팅을 위한 오픈소스 프레임웍

분산 파일 시스템 GFS, 분산 처리 시스템 MapReduce 소프트웨어구현체


hadoop 종류 : 

표준 - 병령 컴퓨팅 실행하는 MapReduce, 분산형 파일 시스템 HDFS, 유틸리티와 라이브러리 Hadoop Common

벤더 배포판 - 클라우데라, 맵알, 호트웍스 / IBM, 인텔, 피보탈 소프트웨어 등 존재

	장점 : 신뢰성 (회사측에서 버그 발생 시 빠르게 대응함), 지원 (기술 지원 서비스를 제공)
	        완성도 (특정 작업을 처리하는 툴로 기능을 보강한 배포판이 많다)


Common : 다른 하둡 모듈을 지원하는 유틸리티

HDFS : Hadoop Distributed File System
         어플리케이션 데이터에 고성능 접근을 지원하는 분산 파일 시스템

MapReduce : 대용량 데이터를 병렬처리 하는 얀 기반 시스템

YARN : 잡 스케줄링과 클러스터 리로스 관리를 위한 프레임웍

HBase : 분산 데이터베이스



-- HDFS : 어플리케이션 기반 파일시스템
	 
	 NameNode와 DataNode로 구성
	
	 Master NameNode : 파일시스템 이미지(fsimage)와 변경기록(edits)을 저장
	 Secondary NameNode : Master Node의 파일들의 사본을 저장
	 DataNode : 데이터파일의 블록을 저장, 디폴트 블록의 크기는 128MB
	 
	 Block(Chunk) 단위로 파일관리(저장/복제/삭제)
	
 
-- YARN : Yet Another Resource Negotiator


-- MapReduce : 소프트웨어 프레임워크, HDFS에 저장된 파일 이용

	map()과 reduce()함수 기반으로 구성

	map() : (key,value)쌍을 처리하여 또 다른 쌍을 생성하는 함수
		
	reduce() : 맵으로부터 생성된 쌍들을 병합하여 최정적으로 list(value)를 생성하는 함수

	java언어가 필요함

	흐름 : 입력 스플릿(data) -> Mapper map() -> shuffle -> Reduer redu



hdfs namenode -format

start-dfs.sh or ./start-dfs.sh

jps

firefox - 127.0.0.1:9870

naver.me/FH7Kduqr


-- Spark : 범용적 목적의 분산 고성능 클러스터링 플랫폼

주요 기능 - Map & Reduce
	Streaming 데이터 핸들링
	SQL 기반의 데이터 쿼리
	머신 러닝 라이브러리
	
	스칼라로 구현되었지만 파이썬,자바,R,스칼라 등 다양한 언어를 지원하기 위한 SDK를 지님

Spark Driver / Workers

Driver : 사용자의 application

Workers : 클러스터 노드 또는 로컬 쓰레드의 프로그램

RDD : 


-----------------------------------------------------------------------

-- 파일 업로드 후 데이터노드에서 블록 확인

bunzip2 2008~  : 파일 압축 해제


hdfs dfs -mkdir /airline/  : 디렉토리 생성

hdfs dfs -put 2008.csv /airline/ : 디렉토리에 파일 업로드

hdfs dfs -ls /airline

업로드한 파일 위치 - http://namenode-ip:9870 (192.168.100.150:9870)


-- 데이터 노드의 블록 확인

ssh slave1

cd /dfs/data/current/BP-xxx/current/finalized/subdir0

ls -al

----------------------------------------------------------------------------------------------

-- spark 설치 및 설정

spark.apahce.org/downloads.html


mv sparkXXXXX.tgz ../
cd
ll
tar -xvf sparxxxxx spark-3.1.3
cd spark-3.1.3/conf
cp spark-env.sh.tmplate spark-env.sh
vi spark-env.sh
	export SPARK_DIST_CLASSPATH=$(~/Downloads/hadoop-3.2.2/bin/hadoop classpath)
	export JAVA_HOME=~/Downloads/jdk1.8.0_321
	
cp log4j.properties.template log4j.properties
vi log4j.properties
	log4j.rootCategory=WARN console


-- spark 실행

~/spark-3.2.1/sbin/start-all.sh
./start-all.sh
jps


-----------------------------------------------------------------------

-- pyspark 실행

python 설치

dnf install python3.8 
apt install python3.8
alternatives --set python /usr/bin/python3

python3

print("hello")

exit()


cd spark~/bin

ll

./pyspark

~/spark-3.2.1/bin/spark-sql


apt install python3-pip

python -m pip install notebook



vi .bashrc
	export PYSPARK_DRIVER_PYTHON=jupyter
	export PYSPARK_DRIVER_PYTHON_OPTS=notebook
source .bashrc




